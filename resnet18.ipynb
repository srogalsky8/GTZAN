{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "# Basic block for ResNet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)  # 1 input channel\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(8960 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 218, 335])\n",
      "Number of channels: 1\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "resize_scale = 0.5\n",
    "\n",
    "def custom_transform(image):\n",
    "    # Crop the image to the desired region of interest (ROI)\n",
    "    image = image.crop((55, 35, 390, 253))\n",
    "    # Convert the cropped image to a PyTorch tensor\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "def load_data(resize_images = False):\n",
    "    # Define your data transformation (without resizing)\n",
    "    transforms_list = []\n",
    "    transforms_list.append(transforms.Grayscale(num_output_channels=1))\n",
    "    transforms_list.append(transforms.Lambda(custom_transform))  # Apply the custom transformation\n",
    "    if resize_images:\n",
    "        transforms_list.append(transforms.Resize((int(218*resize_scale), int(335*resize_scale))))\n",
    "    transforms_list.append(transforms.Normalize((0.5,), (0.5,)))  # Normalize to [-1, 1]\n",
    "    data_transform = transforms.Compose(transforms_list)\n",
    "\n",
    "    class CustomImageDataset(datasets.ImageFolder):\n",
    "        def __init__(self, root, transform=None):\n",
    "            super(CustomImageDataset, self).__init__(root=root, transform=transform)\n",
    "\n",
    "    # Define the path to your data folder\n",
    "    data_dir = 'data/images_original'\n",
    "\n",
    "    # Create an instance of your custom dataset\n",
    "    custom_dataset = CustomImageDataset(root=data_dir, transform=data_transform)\n",
    "\n",
    "    # Calculate the size of the training and testing sets\n",
    "    total_size = len(custom_dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])\n",
    "    batch_size = 10  # You can adjust this batch size as needed\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_data(resize_images=False)\n",
    "\n",
    "# Get a batch of data from the training loader\n",
    "data_iterator = iter(train_loader)\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "print(\"Image shape:\", images[0].shape)\n",
    "num_channels = images[0].shape[0]  # The number of channels in the image\n",
    "print(\"Number of channels:\", num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 192.194 | Accuracy: 33.00%\n",
      "Epoch 2 | Loss: 131.885 | Accuracy: 36.00%\n",
      "Epoch 3 | Loss: 115.725 | Accuracy: 47.50%\n",
      "Epoch 4 | Loss: 79.538 | Accuracy: 54.50%\n",
      "Epoch 5 | Loss: 71.702 | Accuracy: 59.00%\n",
      "Epoch 6 | Loss: 56.326 | Accuracy: 59.00%\n",
      "Epoch 7 | Loss: 57.749 | Accuracy: 53.00%\n",
      "Epoch 8 | Loss: 37.454 | Accuracy: 62.00%\n",
      "Epoch 9 | Loss: 24.690 | Accuracy: 64.00%\n",
      "Epoch 10 | Loss: 21.647 | Accuracy: 67.50%\n",
      "Epoch 11 | Loss: 13.959 | Accuracy: 62.00%\n",
      "Epoch 12 | Loss: 10.079 | Accuracy: 66.50%\n",
      "Epoch 13 | Loss: 6.870 | Accuracy: 65.00%\n",
      "Epoch 14 | Loss: 5.986 | Accuracy: 67.00%\n",
      "Epoch 15 | Loss: 4.197 | Accuracy: 65.00%\n",
      "Epoch 16 | Loss: 5.403 | Accuracy: 64.00%\n",
      "Epoch 17 | Loss: 3.599 | Accuracy: 68.00%\n",
      "Epoch 18 | Loss: 2.678 | Accuracy: 63.50%\n",
      "Epoch 19 | Loss: 2.276 | Accuracy: 67.50%\n",
      "Epoch 20 | Loss: 2.736 | Accuracy: 67.50%\n",
      "Epoch 21 | Loss: 2.206 | Accuracy: 68.50%\n",
      "Epoch 22 | Loss: 2.845 | Accuracy: 67.50%\n",
      "Epoch 23 | Loss: 2.021 | Accuracy: 64.50%\n",
      "Epoch 24 | Loss: 1.874 | Accuracy: 66.50%\n",
      "Epoch 25 | Loss: 2.118 | Accuracy: 66.00%\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your neural network\n",
    "net = ResNet18()\n",
    "net.to(device)\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0003, momentum=0.9)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 25\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over the training dataset\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model on the test dataset and calculate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1} | Loss: {running_loss:.3f} | Accuracy: {test_accuracy:.2f}%\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "print(\"Training finished\")\n",
    "\n",
    "# Save the trained model if desired\n",
    "# torch.save(net.state_dict(), \"my_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define your neural network\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m net \u001b[39m=\u001b[39m ResNet18()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m net\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/m298rogalskyproject/resnet18.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Define the loss function (criterion) and optimizer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet18' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Define your neural network\n",
    "net = ResNet18()\n",
    "net.to(device)\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-6, momentum=0.9)\n",
    "\n",
    "# Define a learning rate range\n",
    "start_lr = 1e-7  # Lower starting learning rate\n",
    "end_lr = 1.0  # Wider range\n",
    "num_steps = 10  # Fewer steps\n",
    "\n",
    "# Create a learning rate scheduler with logarithmic interpolation\n",
    "lr_lambda = lambda step: 10 ** (start_lr + (end_lr - start_lr) * step / (num_steps - 1))\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Lists to store learning rates and losses\n",
    "learning_rates = []\n",
    "losses = []\n",
    "\n",
    "# Set the number of training iterations for each learning rate step\n",
    "num_iterations = 20  # A small number of minibatches\n",
    "\n",
    "# Counter to keep track of the current step\n",
    "step_counter = 0\n",
    "\n",
    "# Learning rate range test loop\n",
    "for step in range(num_steps):\n",
    "    # Set the learning rate using the scheduler\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Create a random subset from the training dataset\n",
    "    random_subset = random.sample(range(len(train_loader.dataset)), num_iterations * train_loader.batch_size)\n",
    "\n",
    "    for i in random_subset:\n",
    "        inputs, labels = train_loader.dataset[i]  # Assuming a tuple structure\n",
    "\n",
    "        # Convert inputs and labels to tensors and move to device\n",
    "        inputs = torch.tensor(inputs).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.unsqueeze(0))\n",
    "        loss = criterion(outputs, labels.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Record the loss for this step\n",
    "    losses.append(running_loss / (num_iterations * train_loader.batch_size))\n",
    "\n",
    "    # Increment the step counter\n",
    "    step_counter += 1\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Step {step_counter}/{num_steps} | Learning Rate: {current_lr} | Loss: {running_loss / (num_iterations * train_loader.batch_size)}\")\n",
    "\n",
    "# Plot the learning rate vs. loss curve\n",
    "plt.plot(learning_rates, losses)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal learning rate\n",
    "optimal_lr = learning_rates[losses.index(min(losses))]\n",
    "print(f\"Optimal Learning Rate: {optimal_lr}\")\n",
    "\n",
    "# Now you can use the optimal learning rate for training\n",
    "# Update the learning rate in your optimizer with the found optimal_lr\n",
    "# optimizer = optim.SGD(net.parameters(), lr=optimal_lr, momentum=0.9)\n",
    "\n",
    "# # Continue with your training loop using the optimal learning rate\n",
    "# for epoch in range(1):  # You can set the number of full training epochs here\n",
    "#     # Rest of your training code\n",
    "#     # ...\n",
    "\n",
    "# print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07\n",
      "5.99484250318941e-07\n",
      "3.593813663804627e-06\n",
      "2.154434690031884e-05\n",
      "0.0001291549665014884\n",
      "0.0007742636826811272\n",
      "0.00464158883361278\n",
      "0.02782559402207125\n",
      "0.16681005372000593\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "start_lr = 1e-7  # Lower starting learning rate\n",
    "end_lr = 1  # Wider range\n",
    "num_steps = 10  # Fewer steps\n",
    "\n",
    "# Calculate the scaling factor for each step\n",
    "scaling_factor = math.exp(math.log(end_lr / start_lr) / (num_steps - 1))\n",
    "\n",
    "lr_lambda = lambda step: start_lr * (scaling_factor ** step)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    print(lr_lambda(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
